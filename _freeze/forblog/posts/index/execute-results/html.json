{
  "hash": "ce1fac8fe9406fb14b07371962bea8db",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Deep Neural Networks: Mathematical Foundations and Architectures\"\nauthor: \"Antonio O.R.\"\ndescription: \"Mathematical foundations of deep learning with focus on Physics-Informed Neural Networks\"\nformat:\n  html:\n    theme: flatly\n    toc: true\n    toc-depth: 3\n    code-fold: true\n    code-tools: true\n    highlight-style: github\n    \nexecute:\n  echo: true\n  warning: false\njupyter: python3\n---\n\n# 1. Shallow neural networks\n\nLet $\\text{x}=(x_1,...,x_n)\\in \\mathbb{R}^n$ be a multivariate input and $\\text{y}=(y_1,...,y_m)\\in \\mathbb{R}^m$ a multivariate output $(n,m>0)$.\nShallow neural networks are functions with parameters\n\n\\begin{align*}\n\\phi=\\{\\phi_{10},...,\\phi_{1d},..., \\phi_{m0},...,\\phi_{md},\\theta_{10},..., \\theta_{d0},...,\\theta_{1n},..., \\theta_{dn}\\},\n\\end{align*}\n\nwhere $d$ is the number of activation functions a[•].\n\nCase $n=m=1, d=3$:\n\n\\begin{align*}\ny &= f[x, \\boldsymbol{\\phi}] \\\\\n  &= \\phi_0 + \\phi_1 a [\\theta_{10} + \\theta_{11} x] + \\phi_2 a [\\theta_{20} + \\theta_{21} x] + \\phi_3 a [\\theta_{30} + \\theta_{31} x].\n\\end{align*}\n\n![](images/ShallowNet.svg)\n\nWe can break down this calculation into three parts:\n\n\n1.   Compute three linear functions of the input data $(\\theta_{10} + \\theta_{11} x, \\theta_{20} + \\theta_{21} x, \\theta_{30} + \\theta_{31} x)$\n2.   Pass the three results through an activation function a[•]\n3. Weight the three resulting activations with $\\theta_1$ , $\\theta_2$ , and $\\theta_3$ , sum them, and add an offset $\\theta_0$.\n\n\nTo complete the description, we must define the activation function a[•]. There are many possibilities, but the hyperbolic tangent function is commonly used as an activation function in Physics-Informed Neural Networks (PINNs) due to its smooth and differentiable nature.\n\n\\begin{align*}\na[z] &= \\tanh[z] =\n\\begin{cases}\n-1 & z \\ll 0 \\\\\nz & |z| \\approx 0 \\\\\n1 & z \\gg 0\n\\end{cases}\n\\end{align*}\n\nSome advantages of using $\\tanh$ in PINNs include:\n\n\n*    **Smoothness**: Unlike ReLU, $\\tanh$ is infinitely differentiable, which is beneficial for enforcing physical constraints that involve higher-order derivatives.\n*   **Symmetry**: It is symmetric around the origin, making it useful for capturing variations in both positive and negative directions.\n*    **Better Gradient Flow**: Compared to sigmoid, $\\tanh$ has a steeper gradient, reducing the risk of vanishing gradients in deep networks.\n*    **Physical Interpretability**: In many physical systems, solutions naturally exhibit smooth transitions, which $\\tanh$ can better approximate.\n\n::: {.callout-tip collapse=\"true\"}\n## Metacognitive Insight\nThis section masterfully bridges theoretical foundations (parameterized functions) with practical considerations (activation function selection), demonstrating how mathematical abstraction serves applied goals. The deliberate decomposition and domain-aware justification reveal expert knowledge organization - transforming complex concepts into teachable components while maintaining scientific rigor.\n:::\n\n# 2. Deep neural networks\n\n### General formulation\n\nWe will describe the vector of hidden units at layer $k$ as $\\mathbf{h}_k$,\nthe vector of biases (intercepts) that contribute to hidden layer $k+1$ as $\\boldsymbol{\\beta}_k$,\nand the weights (slopes) that are applied to the $k^{th}$ layer and contribute\nto the $(k+1)^{th}$ layer as $\\boldsymbol{\\Omega}_k$. A general deep network $\\mathbf{y} = f[\\mathbf{x}, \\phi]$ with $K$ layers\ncan now be written as:\n\n\n\\begin{aligned}\n    \\mathbf{h}_1 &= a[\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}] \\\\\n    \\mathbf{h}_2 &= a[\\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{h}_1] \\\\\n    \\mathbf{h}_3 &= a[\\boldsymbol{\\beta}_2 + \\boldsymbol{\\Omega}_2 \\mathbf{h}_2] \\\\\n    &\\vdots \\\\\n    \\mathbf{h}_K &= a[\\boldsymbol{\\beta}_{K-1} + \\boldsymbol{\\Omega}_{K-1} \\mathbf{h}_{K-1}] \\\\\n    \\mathbf{y} &= \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K \\mathbf{h}_K.\n\\end{aligned}\n\n\n\nThe parameters $\\phi$ of this model comprise all of these weight matrices and bias vectors:\n\n\\begin{aligned}\n\\phi = \\{\\beta_k, \\Omega_k\\}_{k=0}^{K}.\n\\end{aligned}\n\nIf the $k^{th}$ layer has $D_k$ hidden units, then the bias vector $\\boldsymbol{\\beta}_{k-1}$ will be of size $D_k$. The last bias vector $\\boldsymbol{\\beta}_k$ has the size $D_o$ of the output. The first weight matrix $\\boldsymbol{\\Omega}_0$ has size $D_1×D_i$, where $D_i$ is the size of the input. The last weight matrix **Ωₖ** is *D₀ × Dₖ*, and the remaining matrices $\\boldsymbol{\\Omega}_k$ are $D_{k+1}×D_k$ (figure 4.6).\n\nWe can equivalently write the network as a single function:\n\n\\begin{aligned}\n    \\mathbf{y} &= \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K a\n    \\left[\\boldsymbol{\\beta}_{K-1} + \\boldsymbol{\\Omega}_{K-1}\n    a \\left[\\dots \\boldsymbol{\\beta}_2 + \\boldsymbol{\\Omega}_2\n    a \\left[\\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1\n    a [\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}]\n    \\right] \\dots \\right] \\right].\n\\end{aligned}\n\n![](images/DeepNet.svg)\n\n## Shallow vs. deep neural networks\n\n### Advantages of Deep Neural Networks for PINNs\n\nPhysics-Informed Neural Networks (PINNs) benefit significantly from deep architectures due to the following advantages:\n\n#### Superior Function Approximation  \n* Deep networks can approximate complex physical functions by leveraging their ability to represent compositions of simpler functions.  \n* This hierarchical representation aligns well with the multi-scale nature of many physical processes.  \n\n#### Higher Expressiveness with Fewer Parameters  \n* Deep networks create significantly more linear regions than shallow networks with the same parameter count.  \n* This allows PINNs to capture intricate solution structures more efficiently.  \n* The increased expressiveness is particularly useful for solving PDEs with sharp gradients or discontinuities.  \n\n#### Depth Efficiency in Learning Complex Physics  \n* Some physical problems require exponentially more neurons in a shallow network to match the performance of a deep network.  \n* Deep architectures can learn structured physical relationships with fewer hidden units, making training more efficient.  \n\n#### Handling High-Dimensional and Structured Inputs  \n* Many physical problems involve high-dimensional inputs (e.g., spatiotemporal fields).  \n* Deep networks can efficiently process local information and integrate it into a global understanding, mimicking numerical solvers.  \n* This property is crucial when solving PDEs over complex domains, as deep networks can better capture spatial correlations.  \n\n![](images/DeepVsShallow.svg)\n\n::: {.callout-tip collapse=\"true\"}\n## Metacognitive Insight\nThe deep neural network formulation demonstrates expert-level knowledge organization by: (1) systematically decomposing hierarchical transformations through layer-wise notation, (2) explicitly tracking parameter dimensions to reinforce computational intuition, and (3) contrasting architectures to highlight inductive biases. The PINN-specific advantages reveal deep domain awareness - connecting abstract network properties to concrete physical modeling requirements through multi-scale reasoning and parameter efficiency arguments.\n:::\n\n# 3. Loss functions\n\nConsider a model $f[\\mathbf{x}, \\phi]$. Until now, we have\nimplied that the model directly computes a prediction $\\mathbf{y}$. We now shift perspective and\nconsider the model as computing a conditional probability distribution $P(\\mathbf{y}|\\mathbf{x})$. The loss encourages each training output $y_i$ to have\na high probability under the distribution $P(y_j |x_i )$.\n\n![](images/Loss.svg)\n\n## 3.1. How a model $f[\\mathbf{x}, \\phi]$ can be adapted to compute a probability distribution?\n\n1.   Choose a parametric distribution $P(\\mathbf{y}|\\theta)$ defined on the output domain y,\n2.   Use the network to compute one or more of the parameters $\\theta$ of this distribution.\n\nFor example, suppose the prediction domain is the set of real numbers, so $y\\in\\mathbb{R}$.\nHere, we might choose the univariate normal distribution, which is defined on $\\mathbb{R}$. This\ndistribution is defined by the mean µ and variance $\\sigma^2$ , so $\\theta = \\{\\mu, \\sigma^2\\}$. The machine\nlearning model might predict the mean $\\mu$, and the variance $\\sigma^2$ could be treated as an\nunknown constant.\n\n## 3. 2. Maximum likelihood criterion\n\nThe model now computes different distribution parameters $ \\theta_i = f[x_i , \\phi]$ for each training\ninput $x_i$ . Each observed training output $y_i$ should have high probability under its\ncorresponding distribution $P(y_i |\\theta_i )$. Hence, we choose the model parameters $\\phi$ so that\nthey maximize the combined probability across all $I$ training examples:\n\n\\begin{align*}\n\\hat{\\boldsymbol{\\phi}} &= \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmax}}\\left[\\prod_{i=1}^I P(y_i|x_i)\\right]\\\\\n&= \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmax}}\\left[\\prod_{i=1}^I P(y_i|\\boldsymbol{\\theta}_i)\\right]\\\\\n&= \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmax}}\\left[\\prod_{i=1}^I P(y_i|f[x_i,\\phi])\\right].\n\\end{align*}\n\nHere we are implicitly assuming that the data $\\{x_i,y_i\\}_{i=1}^I$ are $independent$ and $identically$ $distributed$ $(i.i.d.)$.\n\n\\begin{align*}\nPr(y_1,y_2,\\ldots,y_I|x_1,x_2,\\ldots,x_I) = \\prod_{i=1}^I Pr(y_i|x_i)\n\\end{align*}\n\nWe can equivalently maximize the logarithm of the likelihood:\n\n\\begin{align*}\n\\hat{\\phi} & = \\operatorname*{argmax}_{\\phi} \\left[ \\prod_{i=1}^{I} P(y_i|f[x_i,\\phi]) \\right] \\\\\n& = \\operatorname*{argmax}_{\\phi} \\left[ \\log\\left[ \\prod_{i=1}^{I} P(y_i|f[x_i,\\phi]) \\right] \\right] \\\\\n& = \\operatorname*{argmax}_{\\phi} \\left[ \\sum_{i=1}^{I} \\log\\left[ P(y_i|f[x_i,\\phi]) \\right] \\right].\n\\end{align*}\n\nBy convention, model fitting problems are framed in terms of\nminimizing a loss. To convert the maximum log-likelihood criterion to a minimization\nproblem, we multiply by minus one, which gives us the negative $log$-$likelihood$ $criterion$:\n\n\\begin{align*}\n\\hat{\\boldsymbol{\\phi}} &\\ =\\ \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^I\\mathrm{log}\\Bigl[P(y_i|f[x_i,\\phi])\\Bigr]\\right]\\\\\n&=\\ \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}}\\Bigl[L[\\phi]\\Bigr],\n\\end{align*}\n\nwhich is what forms the final loss function $L[\\phi]$.\n\n## 3. 3. Recipe for constructing loss functions\n\nThe recipe for constructing loss functions for training data $\\{x_i , y_i \\}$ using the maximum likelihood approach is hence:\n\n1.   Choose a suitable probability distribution $P(\\mathbf{y}|\\theta)$ defined over the domain of the predictions $\\mathbf{y}$ with distribution parameters $\\theta$.\n\n![](images/ConstructLoss.png)\n\n2. Set the machine learning model $f[\\mathbf{x}, \\theta]$ to predict one or more of these parameters, so $\\theta = f[\\mathbf{x}, \\theta]$ and $P(y|θ) = P(y|f[\\mathbf{x}, \\theta])$.\n\n3. To train the model, find the network parameters $\\hat{\\phi}$ that minimize the negative\nlog-likelihood loss function over the training dataset pairs $\\{x_i , y_i \\}$:\n\n\\begin{align*}\n\\hat{\\phi} = \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}}\\Bigl[L[\\phi]\\Bigr] = \\operatorname*{argmin}_{\\phi} \\left[ - \\sum_{i=1}^I \\log \\left[ Pr(y_i | f[x_i, \\phi]) \\right] \\right].\n\\end{align*}\n\n4. To perform inference for a new test example $\\mathbf{x}$, return either the full distribution $P(\\mathbf{y}|f[\\mathbf{x}, \\phi])$ or the value where this distribution is maximized.\n\n::: {.callout-tip collapse=\"true\"}\n## Metacognitive Insight\nThis section elegantly bridges probabilistic thinking with neural network training by: (1) framing predictions as distributions rather than point estimates, (2) demonstrating how log transformations convert products to more tractable sums, and (3) providing a clear 4-step recipe that connects theoretical probability to practical implementation. The i.i.d. assumption is crucially highlighted as it underpins the factorization enabling efficient optimization.\n:::\n\n# 4. Example: univariate regression\n\nThe goal is to predict a single scalar output $y\\in\\mathbb{R}$ from input $\\mathbf{x}$ using a model $f[\\mathbf{x},\\phi]$ with parameters $\\phi$. We select the univariate normal , which is defined over $\\mathbf{y}\\in\\mathbb{R}$. This distribution has two parameters (mean $\\mu$ and variance $\\sigma^{2}$) and has a probability density function:\n\n\\begin{align*}\nPr(y|\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\sigma^{2}} \\exp\\left[-\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}\\right].\n\\end{align*}\n\nSecond, we set the machine learning model $f[\\mathbf{x},\\phi]$ to compute one or more of the parameters of this distribution. Here, we just compute the mean so $\\mu = f[\\mathbf{x},\\phi]$:\n\n\\begin{align*}\nPr(y|f[\\mathbf{x},\\phi],\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\sigma^{2}} \\exp\\left[-\\frac{(y-f[\\mathbf{x},\\phi])^{2}}{2\\sigma^{2}}\\right].\n\\end{align*}\n\nWe aim to find the parameters $\\phi$ that make the training data $\\{x_{i},y_{i}\\}$ most probable under this distribution. To accomplish this, we choose a loss function $L[\\boldsymbol{\\phi}]$ based on the negative log-likelihood:\n\n\\begin{align*}\nL[\\boldsymbol{\\phi}] = -\\sum_{i=1}^{I} \\log\\left[Pr(y_{i}|f[\\mathbf{x},\\phi],\\sigma^{2})\\right]\n\\end{align*}\n\n\\begin{align*}\n= -\\sum_{i=1}^{I} \\log\\left[\\frac{1}{\\sqrt{2\\pi}\\sigma^{2}} \\exp\\left[-\\frac{(y_{i}-f[\\mathbf{x},\\phi])^{2}}{2\\sigma^{2}}\\right]\\right].\n\\end{align*}\n\nWhen we train the model, we seek parameters $\\hat{\\phi}$ that minimize this loss.\n\nThe goal is to predict a single scalar output $y\\in\\mathbb{R}$ from input $\\mathbf{x}$ using a model $f[\\mathbf{x},\\phi]$ with parameters $\\phi$. We select the univariate normal , which is defined over $\\mathbf{y}\\in\\mathbb{R}$. This distribution has two parameters (mean $\\mu$ and variance $\\sigma^{2}$) and has a probability density function:\n\n\\begin{align*}\nPr(y|\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\sigma^{2}} \\exp\\left[-\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}\\right].\n\\end{align*}\n\nSecond, we set the machine learning model $f[\\mathbf{x},\\phi]$ to compute one or more of the parameters of this distribution. Here, we just compute the mean so $\\mu = f[\\mathbf{x},\\phi]$:\n\n\\begin{align*}\nPr(y|f[\\mathbf{x},\\phi],\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\sigma^{2}} \\exp\\left[-\\frac{(y-f[\\mathbf{x},\\phi])^{2}}{2\\sigma^{2}}\\right].\n\\end{align*}\n\nWe aim to find the parameters $\\phi$ that make the training data $\\{x_{i},y_{i}\\}$ most probable under this distribution. To accomplish this, we choose a loss function $L[\\boldsymbol{\\phi}]$ based on the negative log-likelihood:\n\n\\begin{align*}\nL[\\boldsymbol{\\phi}] = -\\sum_{i=1}^{I} \\log\\left[Pr(y_{i}|f[\\mathbf{x},\\phi],\\sigma^{2})\\right]\n\\end{align*}\n\n\\begin{align*}\n= -\\sum_{i=1}^{I} \\log\\left[\\frac{1}{\\sqrt{2\\pi}\\sigma^{2}} \\exp\\left[-\\frac{(y_{i}-f[\\mathbf{x},\\phi])^{2}}{2\\sigma^{2}}\\right]\\right].\n\\end{align*}\n\nWhen we train the model, we seek parameters $\\hat{\\phi}$ that minimize this loss.\n\n## 4.1 Least squares loss function\n\nNow let's perform some algebraic manipulations on the loss function. We seek:\n\n\\begin{align*}\n\\hat{\\phi} = \\operatorname*{argmin}_{\\phi}\\left[-\\sum_{i=1}^{I}\\log\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\frac{(y_{i}-f[x_{i},\\phi])^{2}}{2\\sigma^{2}}\\right]\\right]\\right]\n\\end{align*}\n\n\\begin{align*}\n= \\operatorname*{argmin}_{\\phi}\\left[-\\sum_{i=1}^{I}\\left(\\log\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\right]-\\frac{(y_{i}-f[x_{i},\\phi])^{2}}{2\\sigma^{2}}\\right)\\right]\n\\end{align*}\n\n\\begin{align*}\n= \\operatorname*{argmin}_{\\phi}\\left[-\\sum_{i=1}^{I}\\frac{(y_{i}-f[x_{i},\\phi])^{2}}{2\\sigma^{2}}\\right]\n\\end{align*}\n\n\\begin{align*}\n= \\operatorname*{argmin}_{\\phi}\\left[\\sum_{i=1}^{I}(y_{i}-f[x_{i},\\phi])^{2}\\right],\n\\end{align*}\n\nwhere we removed the first term between the second and third lines because it doesn't depend on $\\phi$. We removed the denominator between the third and fourth lines, as this is just a constant positive scaling factor that does not affect the position of the minimum.\n\nThe result of these manipulations is the least squares loss function that we originally introduced when we discussed linear regression in chapter 2:\n\n\\begin{align*}\nL[\\phi] = \\sum_{i=1}^{I}(y_{i}-f[x_{i},\\phi])^{2}.\n\\end{align*}\n\nWe see that the least squares loss function follows naturally from the assumptions that the predictions are (i) independent and (ii) drawn from a normal distribution with mean $\\mu = f[x_{i},\\phi]$.\n\n![](images/LeastSquaresLoss.svg)\n\n::: {.callout-tip collapse=\"true\"}\n## Metacognitive Insight\nThis example brilliantly connects probability theory with practical regression by: (1) showing how Gaussian assumptions naturally lead to least squares, (2) demonstrating careful mathematical reasoning through term elimination (constants don't affect optimization), and (3) reinforcing the conceptual link between loss functions and probability distributions. The visual representation of the loss function grounds the abstract mathematics in concrete intuition.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}