<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Antonio O.R.</title>
<link>https://toniortiz18.github.io/portfolio/forblog/</link>
<atom:link href="https://toniortiz18.github.io/portfolio/forblog/index.xml" rel="self" type="application/rss+xml"/>
<description>Antonio Ortiz Romero</description>
<image>
<url>https://toniortiz18.github.io/portfolio/images/profile.png</url>
<title>Antonio O.R.</title>
<link>https://toniortiz18.github.io/portfolio/forblog/</link>
</image>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Mon, 31 Mar 2025 12:09:28 GMT</lastBuildDate>
<item>
  <title>Introduction to Deep Neural Networks: Mathematical Foundations and Architectures</title>
  <dc:creator>Antonio O.R.</dc:creator>
  <link>https://toniortiz18.github.io/portfolio/forblog/posts/</link>
  <description><![CDATA[ 




<section id="shallow-neural-networks" class="level1">
<h1>1. Shallow neural networks</h1>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bx%7D=(x_1,...,x_n)%5Cin%20%5Cmathbb%7BR%7D%5En"> be a multivariate input and <img src="https://latex.codecogs.com/png.latex?%5Ctext%7By%7D=(y_1,...,y_m)%5Cin%20%5Cmathbb%7BR%7D%5Em"> a multivariate output <img src="https://latex.codecogs.com/png.latex?(n,m%3E0)">. Shallow neural networks are functions with parameters</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cphi=%5C%7B%5Cphi_%7B10%7D,...,%5Cphi_%7B1d%7D,...,%20%5Cphi_%7Bm0%7D,...,%5Cphi_%7Bmd%7D,%5Ctheta_%7B10%7D,...,%20%5Ctheta_%7Bd0%7D,...,%5Ctheta_%7B1n%7D,...,%20%5Ctheta_%7Bdn%7D%5C%7D,%0A%5Cend%7Balign*%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?d"> is the number of activation functions a[•].</p>
<p>Case <img src="https://latex.codecogs.com/png.latex?n=m=1,%20d=3">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Ay%20&amp;=%20f%5Bx,%20%5Cboldsymbol%7B%5Cphi%7D%5D%20%5C%5C%0A%20%20&amp;=%20%5Cphi_0%20+%20%5Cphi_1%20a%20%5B%5Ctheta_%7B10%7D%20+%20%5Ctheta_%7B11%7D%20x%5D%20+%20%5Cphi_2%20a%20%5B%5Ctheta_%7B20%7D%20+%20%5Ctheta_%7B21%7D%20x%5D%20+%20%5Cphi_3%20a%20%5B%5Ctheta_%7B30%7D%20+%20%5Ctheta_%7B31%7D%20x%5D.%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://toniortiz18.github.io/portfolio/forblog/posts/images/ShallowNet.svg" class="img-fluid"></p>
<p>We can break down this calculation into three parts:</p>
<ol type="1">
<li>Compute three linear functions of the input data <img src="https://latex.codecogs.com/png.latex?(%5Ctheta_%7B10%7D%20+%20%5Ctheta_%7B11%7D%20x,%20%5Ctheta_%7B20%7D%20+%20%5Ctheta_%7B21%7D%20x,%20%5Ctheta_%7B30%7D%20+%20%5Ctheta_%7B31%7D%20x)"></li>
<li>Pass the three results through an activation function a[•]</li>
<li>Weight the three resulting activations with <img src="https://latex.codecogs.com/png.latex?%5Ctheta_1"> , <img src="https://latex.codecogs.com/png.latex?%5Ctheta_2"> , and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_3"> , sum them, and add an offset <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0">.</li>
</ol>
<p>See https://udlbook.github.io/udlfigures/.</p>
<p>To complete the description, we must define the activation function a[•]. There are many possibilities, but the hyperbolic tangent function is commonly used as an activation function in Physics-Informed Neural Networks (PINNs) due to its smooth and differentiable nature.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Aa%5Bz%5D%20&amp;=%20%5Ctanh%5Bz%5D%20=%0A%5Cbegin%7Bcases%7D%0A-1%20&amp;%20z%20%5Cll%200%20%5C%5C%0Az%20&amp;%20%7Cz%7C%20%5Capprox%200%20%5C%5C%0A1%20&amp;%20z%20%5Cgg%200%0A%5Cend%7Bcases%7D%0A%5Cend%7Balign*%7D"></p>
<p>Some advantages of using <img src="https://latex.codecogs.com/png.latex?%5Ctanh"> in PINNs include:</p>
<ul>
<li><strong>Smoothness</strong>: Unlike ReLU, <img src="https://latex.codecogs.com/png.latex?%5Ctanh"> is infinitely differentiable, which is beneficial for enforcing physical constraints that involve higher-order derivatives.</li>
<li><strong>Symmetry</strong>: It is symmetric around the origin, making it useful for capturing variations in both positive and negative directions.</li>
<li><strong>Better Gradient Flow</strong>: Compared to sigmoid, <img src="https://latex.codecogs.com/png.latex?%5Ctanh"> has a steeper gradient, reducing the risk of vanishing gradients in deep networks.</li>
<li><strong>Physical Interpretability</strong>: In many physical systems, solutions naturally exhibit smooth transitions, which <img src="https://latex.codecogs.com/png.latex?%5Ctanh"> can better approximate.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metacognitive Insight
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This section masterfully bridges theoretical foundations (parameterized functions) with practical considerations (activation function selection), demonstrating how mathematical abstraction serves applied goals. The deliberate decomposition and domain-aware justification reveal expert knowledge organization - transforming complex concepts into teachable components while maintaining scientific rigor.</p>
</div>
</div>
</div>
</section>
<section id="deep-neural-networks" class="level1">
<h1>2. Deep neural networks</h1>
<section id="general-formulation" class="level3">
<h3 class="anchored" data-anchor-id="general-formulation">General formulation</h3>
<p>We will describe the vector of hidden units at layer <img src="https://latex.codecogs.com/png.latex?k"> as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bh%7D_k">, the vector of biases (intercepts) that contribute to hidden layer <img src="https://latex.codecogs.com/png.latex?k+1"> as <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5Cbeta%7D_k">, and the weights (slopes) that are applied to the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> layer and contribute to the <img src="https://latex.codecogs.com/png.latex?(k+1)%5E%7Bth%7D"> layer as <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5COmega%7D_k">. A general deep network <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f%5B%5Cmathbf%7Bx%7D,%20%5Cphi%5D"> with <img src="https://latex.codecogs.com/png.latex?K"> layers can now be written as:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%5Cmathbf%7Bh%7D_1%20&amp;=%20a%5B%5Cboldsymbol%7B%5Cbeta%7D_0%20+%20%5Cboldsymbol%7B%5COmega%7D_0%20%5Cmathbf%7Bx%7D%5D%20%5C%5C%0A%20%20%20%20%5Cmathbf%7Bh%7D_2%20&amp;=%20a%5B%5Cboldsymbol%7B%5Cbeta%7D_1%20+%20%5Cboldsymbol%7B%5COmega%7D_1%20%5Cmathbf%7Bh%7D_1%5D%20%5C%5C%0A%20%20%20%20%5Cmathbf%7Bh%7D_3%20&amp;=%20a%5B%5Cboldsymbol%7B%5Cbeta%7D_2%20+%20%5Cboldsymbol%7B%5COmega%7D_2%20%5Cmathbf%7Bh%7D_2%5D%20%5C%5C%0A%20%20%20%20&amp;%5Cvdots%20%5C%5C%0A%20%20%20%20%5Cmathbf%7Bh%7D_K%20&amp;=%20a%5B%5Cboldsymbol%7B%5Cbeta%7D_%7BK-1%7D%20+%20%5Cboldsymbol%7B%5COmega%7D_%7BK-1%7D%20%5Cmathbf%7Bh%7D_%7BK-1%7D%5D%20%5C%5C%0A%20%20%20%20%5Cmathbf%7By%7D%20&amp;=%20%5Cboldsymbol%7B%5Cbeta%7D_K%20+%20%5Cboldsymbol%7B%5COmega%7D_K%20%5Cmathbf%7Bh%7D_K.%0A%5Cend%7Baligned%7D">
<p>The parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> of this model comprise all of these weight matrices and bias vectors:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Cphi%20=%20%5C%7B%5Cbeta_k,%20%5COmega_k%5C%7D_%7Bk=0%7D%5E%7BK%7D.%0A%5Cend%7Baligned%7D">
<p>If the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> layer has <img src="https://latex.codecogs.com/png.latex?D_k"> hidden units, then the bias vector <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5Cbeta%7D_%7Bk-1%7D"> will be of size <img src="https://latex.codecogs.com/png.latex?D_k">. The last bias vector <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5Cbeta%7D_k"> has the size <img src="https://latex.codecogs.com/png.latex?D_o"> of the output. The first weight matrix <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5COmega%7D_0"> has size <img src="https://latex.codecogs.com/png.latex?D_1%C3%97D_i">, where <img src="https://latex.codecogs.com/png.latex?D_i"> is the size of the input. The last weight matrix <strong>Ωₖ</strong> is <em>D₀ × Dₖ</em>, and the remaining matrices <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7B%5COmega%7D_k"> are <img src="https://latex.codecogs.com/png.latex?D_%7Bk+1%7D%C3%97D_k"> (figure 4.6).</p>
<p>We can equivalently write the network as a single function:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%5Cmathbf%7By%7D%20&amp;=%20%5Cboldsymbol%7B%5Cbeta%7D_K%20+%20%5Cboldsymbol%7B%5COmega%7D_K%20a%0A%20%20%20%20%5Cleft%5B%5Cboldsymbol%7B%5Cbeta%7D_%7BK-1%7D%20+%20%5Cboldsymbol%7B%5COmega%7D_%7BK-1%7D%0A%20%20%20%20a%20%5Cleft%5B%5Cdots%20%5Cboldsymbol%7B%5Cbeta%7D_2%20+%20%5Cboldsymbol%7B%5COmega%7D_2%0A%20%20%20%20a%20%5Cleft%5B%5Cboldsymbol%7B%5Cbeta%7D_1%20+%20%5Cboldsymbol%7B%5COmega%7D_1%0A%20%20%20%20a%20%5B%5Cboldsymbol%7B%5Cbeta%7D_0%20+%20%5Cboldsymbol%7B%5COmega%7D_0%20%5Cmathbf%7Bx%7D%5D%0A%20%20%20%20%5Cright%5D%20%5Cdots%20%5Cright%5D%20%5Cright%5D.%0A%5Cend%7Baligned%7D">
<p><img src="https://toniortiz18.github.io/portfolio/forblog/posts/images/DeepNet.svg" class="img-fluid"></p>
</section>
<section id="shallow-vs.-deep-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="shallow-vs.-deep-neural-networks">Shallow vs.&nbsp;deep neural networks</h2>
<section id="advantages-of-deep-neural-networks-for-pinns" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-deep-neural-networks-for-pinns">Advantages of Deep Neural Networks for PINNs</h3>
<p>Physics-Informed Neural Networks (PINNs) benefit significantly from deep architectures due to the following advantages:</p>
<section id="superior-function-approximation" class="level4">
<h4 class="anchored" data-anchor-id="superior-function-approximation">Superior Function Approximation</h4>
<ul>
<li>Deep networks can approximate complex physical functions by leveraging their ability to represent compositions of simpler functions.<br>
</li>
<li>This hierarchical representation aligns well with the multi-scale nature of many physical processes.</li>
</ul>
</section>
<section id="higher-expressiveness-with-fewer-parameters" class="level4">
<h4 class="anchored" data-anchor-id="higher-expressiveness-with-fewer-parameters">Higher Expressiveness with Fewer Parameters</h4>
<ul>
<li>Deep networks create significantly more linear regions than shallow networks with the same parameter count.<br>
</li>
<li>This allows PINNs to capture intricate solution structures more efficiently.<br>
</li>
<li>The increased expressiveness is particularly useful for solving PDEs with sharp gradients or discontinuities.</li>
</ul>
</section>
<section id="depth-efficiency-in-learning-complex-physics" class="level4">
<h4 class="anchored" data-anchor-id="depth-efficiency-in-learning-complex-physics">Depth Efficiency in Learning Complex Physics</h4>
<ul>
<li>Some physical problems require exponentially more neurons in a shallow network to match the performance of a deep network.<br>
</li>
<li>Deep architectures can learn structured physical relationships with fewer hidden units, making training more efficient.</li>
</ul>
</section>
<section id="handling-high-dimensional-and-structured-inputs" class="level4">
<h4 class="anchored" data-anchor-id="handling-high-dimensional-and-structured-inputs">Handling High-Dimensional and Structured Inputs</h4>
<ul>
<li>Many physical problems involve high-dimensional inputs (e.g., spatiotemporal fields).<br>
</li>
<li>Deep networks can efficiently process local information and integrate it into a global understanding, mimicking numerical solvers.<br>
</li>
<li>This property is crucial when solving PDEs over complex domains, as deep networks can better capture spatial correlations.</li>
</ul>
<p><img src="https://toniortiz18.github.io/portfolio/forblog/posts/images/DeepVsShallow.svg" class="img-fluid"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metacognitive Insight
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The deep neural network formulation demonstrates expert-level knowledge organization by: (1) systematically decomposing hierarchical transformations through layer-wise notation, (2) explicitly tracking parameter dimensions to reinforce computational intuition, and (3) contrasting architectures to highlight inductive biases. The PINN-specific advantages reveal deep domain awareness - connecting abstract network properties to concrete physical modeling requirements through multi-scale reasoning and parameter efficiency arguments.</p>
</div>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="loss-functions" class="level1">
<h1>3. Loss functions</h1>
<p>Consider a model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%20%5Cphi%5D">. Until now, we have implied that the model directly computes a prediction <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">. We now shift perspective and consider the model as computing a conditional probability distribution <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7By%7D%7C%5Cmathbf%7Bx%7D)">. The loss encourages each training output <img src="https://latex.codecogs.com/png.latex?y_i"> to have a high probability under the distribution <img src="https://latex.codecogs.com/png.latex?P(y_j%20%7Cx_i%20)">.</p>
<p><img src="https://toniortiz18.github.io/portfolio/forblog/posts/images/Loss.svg" class="img-fluid"></p>
<section id="how-a-model-fmathbfx-phi-can-be-adapted-to-compute-a-probability-distribution" class="level2">
<h2 class="anchored" data-anchor-id="how-a-model-fmathbfx-phi-can-be-adapted-to-compute-a-probability-distribution">3.1. How a model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%20%5Cphi%5D"> can be adapted to compute a probability distribution?</h2>
<ol type="1">
<li>Choose a parametric distribution <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7By%7D%7C%5Ctheta)"> defined on the output domain y,</li>
<li>Use the network to compute one or more of the parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> of this distribution.</li>
</ol>
<p>For example, suppose the prediction domain is the set of real numbers, so <img src="https://latex.codecogs.com/png.latex?y%5Cin%5Cmathbb%7BR%7D">. Here, we might choose the univariate normal distribution, which is defined on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D">. This distribution is defined by the mean µ and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> , so <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5C%7B%5Cmu,%20%5Csigma%5E2%5C%7D">. The machine learning model might predict the mean <img src="https://latex.codecogs.com/png.latex?%5Cmu">, and the variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> could be treated as an unknown constant.</p>
</section>
<section id="maximum-likelihood-criterion" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-criterion">3. 2. Maximum likelihood criterion</h2>
<p>The model now computes different distribution parameters $ _i = f[x_i , ]$ for each training input <img src="https://latex.codecogs.com/png.latex?x_i"> . Each observed training output <img src="https://latex.codecogs.com/png.latex?y_i"> should have high probability under its corresponding distribution <img src="https://latex.codecogs.com/png.latex?P(y_i%20%7C%5Ctheta_i%20)">. Hence, we choose the model parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> so that they maximize the combined probability across all <img src="https://latex.codecogs.com/png.latex?I"> training examples:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Chat%7B%5Cboldsymbol%7B%5Cphi%7D%7D%20&amp;=%20%5Cunderset%7B%5Cboldsymbol%7B%5Cphi%7D%7D%7B%5Cmathrm%7Bargmax%7D%7D%5Cleft%5B%5Cprod_%7Bi=1%7D%5EI%20P(y_i%7Cx_i)%5Cright%5D%5C%5C%0A&amp;=%20%5Cunderset%7B%5Cboldsymbol%7B%5Cphi%7D%7D%7B%5Cmathrm%7Bargmax%7D%7D%5Cleft%5B%5Cprod_%7Bi=1%7D%5EI%20P(y_i%7C%5Cboldsymbol%7B%5Ctheta%7D_i)%5Cright%5D%5C%5C%0A&amp;=%20%5Cunderset%7B%5Cboldsymbol%7B%5Cphi%7D%7D%7B%5Cmathrm%7Bargmax%7D%7D%5Cleft%5B%5Cprod_%7Bi=1%7D%5EI%20P(y_i%7Cf%5Bx_i,%5Cphi%5D)%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>Here we are implicitly assuming that the data <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_i,y_i%5C%7D_%7Bi=1%7D%5EI"> are <img src="https://latex.codecogs.com/png.latex?independent"> and <img src="https://latex.codecogs.com/png.latex?identically"> <img src="https://latex.codecogs.com/png.latex?distributed"> <img src="https://latex.codecogs.com/png.latex?(i.i.d.)">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0APr(y_1,y_2,%5Cldots,y_I%7Cx_1,x_2,%5Cldots,x_I)%20=%20%5Cprod_%7Bi=1%7D%5EI%20Pr(y_i%7Cx_i)%0A%5Cend%7Balign*%7D"></p>
<p>We can equivalently maximize the logarithm of the likelihood:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Chat%7B%5Cphi%7D%20&amp;%20=%20%5Coperatorname*%7Bargmax%7D_%7B%5Cphi%7D%20%5Cleft%5B%20%5Cprod_%7Bi=1%7D%5E%7BI%7D%20P(y_i%7Cf%5Bx_i,%5Cphi%5D)%20%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Coperatorname*%7Bargmax%7D_%7B%5Cphi%7D%20%5Cleft%5B%20%5Clog%5Cleft%5B%20%5Cprod_%7Bi=1%7D%5E%7BI%7D%20P(y_i%7Cf%5Bx_i,%5Cphi%5D)%20%5Cright%5D%20%5Cright%5D%20%5C%5C%0A&amp;%20=%20%5Coperatorname*%7Bargmax%7D_%7B%5Cphi%7D%20%5Cleft%5B%20%5Csum_%7Bi=1%7D%5E%7BI%7D%20%5Clog%5Cleft%5B%20P(y_i%7Cf%5Bx_i,%5Cphi%5D)%20%5Cright%5D%20%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>By convention, model fitting problems are framed in terms of minimizing a loss. To convert the maximum log-likelihood criterion to a minimization problem, we multiply by minus one, which gives us the negative <img src="https://latex.codecogs.com/png.latex?log">-<img src="https://latex.codecogs.com/png.latex?likelihood"> <img src="https://latex.codecogs.com/png.latex?criterion">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Chat%7B%5Cboldsymbol%7B%5Cphi%7D%7D%20&amp;%5C%20=%5C%20%5Cunderset%7B%5Cboldsymbol%7B%5Cphi%7D%7D%7B%5Cmathrm%7Bargmin%7D%7D%5Cleft%5B-%5Csum_%7Bi=1%7D%5EI%5Cmathrm%7Blog%7D%5CBigl%5BP(y_i%7Cf%5Bx_i,%5Cphi%5D)%5CBigr%5D%5Cright%5D%5C%5C%0A&amp;=%5C%20%5Cunderset%7B%5Cboldsymbol%7B%5Cphi%7D%7D%7B%5Cmathrm%7Bargmin%7D%7D%5CBigl%5BL%5B%5Cphi%5D%5CBigr%5D,%0A%5Cend%7Balign*%7D"></p>
<p>which is what forms the final loss function <img src="https://latex.codecogs.com/png.latex?L%5B%5Cphi%5D">.</p>
</section>
<section id="recipe-for-constructing-loss-functions" class="level2">
<h2 class="anchored" data-anchor-id="recipe-for-constructing-loss-functions">3. 3. Recipe for constructing loss functions</h2>
<p>The recipe for constructing loss functions for training data <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_i%20,%20y_i%20%5C%7D"> using the maximum likelihood approach is hence:</p>
<ol type="1">
<li>Choose a suitable probability distribution <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7By%7D%7C%5Ctheta)"> defined over the domain of the predictions <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> with distribution parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
</ol>
<p><img src="https://toniortiz18.github.io/portfolio/forblog/posts/images/ConstructLoss.png" class="img-fluid"></p>
<ol start="2" type="1">
<li><p>Set the machine learning model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%20%5Ctheta%5D"> to predict one or more of these parameters, so <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20f%5B%5Cmathbf%7Bx%7D,%20%5Ctheta%5D"> and <img src="https://latex.codecogs.com/png.latex?P(y%7C%CE%B8)%20=%20P(y%7Cf%5B%5Cmathbf%7Bx%7D,%20%5Ctheta%5D)">.</p></li>
<li><p>To train the model, find the network parameters <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cphi%7D"> that minimize the negative log-likelihood loss function over the training dataset pairs <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_i%20,%20y_i%20%5C%7D">:</p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Chat%7B%5Cphi%7D%20=%20%5Cunderset%7B%5Cboldsymbol%7B%5Cphi%7D%7D%7B%5Cmathrm%7Bargmin%7D%7D%5CBigl%5BL%5B%5Cphi%5D%5CBigr%5D%20=%20%5Coperatorname*%7Bargmin%7D_%7B%5Cphi%7D%20%5Cleft%5B%20-%20%5Csum_%7Bi=1%7D%5EI%20%5Clog%20%5Cleft%5B%20Pr(y_i%20%7C%20f%5Bx_i,%20%5Cphi%5D)%20%5Cright%5D%20%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<ol start="4" type="1">
<li>To perform inference for a new test example <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">, return either the full distribution <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7By%7D%7Cf%5B%5Cmathbf%7Bx%7D,%20%5Cphi%5D)"> or the value where this distribution is maximized.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metacognitive Insight
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This section elegantly bridges probabilistic thinking with neural network training by: (1) framing predictions as distributions rather than point estimates, (2) demonstrating how log transformations convert products to more tractable sums, and (3) providing a clear 4-step recipe that connects theoretical probability to practical implementation. The i.i.d. assumption is crucially highlighted as it underpins the factorization enabling efficient optimization.</p>
</div>
</div>
</div>
</section>
</section>
<section id="example-univariate-regression" class="level1">
<h1>4. Example: univariate regression</h1>
<p>The goal is to predict a single scalar output <img src="https://latex.codecogs.com/png.latex?y%5Cin%5Cmathbb%7BR%7D"> from input <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> using a model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D"> with parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi">. We select the univariate normal , which is defined over <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%5Cin%5Cmathbb%7BR%7D">. This distribution has two parameters (mean <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%7B2%7D">) and has a probability density function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0APr(y%7C%5Cmu,%5Csigma%5E%7B2%7D)%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%5E%7B2%7D%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B(y-%5Cmu)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>Second, we set the machine learning model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D"> to compute one or more of the parameters of this distribution. Here, we just compute the mean so <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0APr(y%7Cf%5B%5Cmathbf%7Bx%7D,%5Cphi%5D,%5Csigma%5E%7B2%7D)%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%5E%7B2%7D%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B(y-f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>We aim to find the parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> that make the training data <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_%7Bi%7D,y_%7Bi%7D%5C%7D"> most probable under this distribution. To accomplish this, we choose a loss function <img src="https://latex.codecogs.com/png.latex?L%5B%5Cboldsymbol%7B%5Cphi%7D%5D"> based on the negative log-likelihood:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AL%5B%5Cboldsymbol%7B%5Cphi%7D%5D%20=%20-%5Csum_%7Bi=1%7D%5E%7BI%7D%20%5Clog%5Cleft%5BPr(y_%7Bi%7D%7Cf%5B%5Cmathbf%7Bx%7D,%5Cphi%5D,%5Csigma%5E%7B2%7D)%5Cright%5D%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A=%20-%5Csum_%7Bi=1%7D%5E%7BI%7D%20%5Clog%5Cleft%5B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%5E%7B2%7D%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B(y_%7Bi%7D-f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>When we train the model, we seek parameters <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cphi%7D"> that minimize this loss.</p>
<p>The goal is to predict a single scalar output <img src="https://latex.codecogs.com/png.latex?y%5Cin%5Cmathbb%7BR%7D"> from input <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> using a model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D"> with parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi">. We select the univariate normal , which is defined over <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%5Cin%5Cmathbb%7BR%7D">. This distribution has two parameters (mean <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%7B2%7D">) and has a probability density function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0APr(y%7C%5Cmu,%5Csigma%5E%7B2%7D)%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%5E%7B2%7D%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B(y-%5Cmu)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>Second, we set the machine learning model <img src="https://latex.codecogs.com/png.latex?f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D"> to compute one or more of the parameters of this distribution. Here, we just compute the mean so <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0APr(y%7Cf%5B%5Cmathbf%7Bx%7D,%5Cphi%5D,%5Csigma%5E%7B2%7D)%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%5E%7B2%7D%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B(y-f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>We aim to find the parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> that make the training data <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_%7Bi%7D,y_%7Bi%7D%5C%7D"> most probable under this distribution. To accomplish this, we choose a loss function <img src="https://latex.codecogs.com/png.latex?L%5B%5Cboldsymbol%7B%5Cphi%7D%5D"> based on the negative log-likelihood:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AL%5B%5Cboldsymbol%7B%5Cphi%7D%5D%20=%20-%5Csum_%7Bi=1%7D%5E%7BI%7D%20%5Clog%5Cleft%5BPr(y_%7Bi%7D%7Cf%5B%5Cmathbf%7Bx%7D,%5Cphi%5D,%5Csigma%5E%7B2%7D)%5Cright%5D%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A=%20-%5Csum_%7Bi=1%7D%5E%7BI%7D%20%5Clog%5Cleft%5B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%5E%7B2%7D%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B(y_%7Bi%7D-f%5B%5Cmathbf%7Bx%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D%5Cright%5D.%0A%5Cend%7Balign*%7D"></p>
<p>When we train the model, we seek parameters <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cphi%7D"> that minimize this loss.</p>
<section id="least-squares-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="least-squares-loss-function">4.1 Least squares loss function</h2>
<p>Now let’s perform some algebraic manipulations on the loss function. We seek:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Chat%7B%5Cphi%7D%20=%20%5Coperatorname*%7Bargmin%7D_%7B%5Cphi%7D%5Cleft%5B-%5Csum_%7Bi=1%7D%5E%7BI%7D%5Clog%5Cleft%5B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E%7B2%7D%7D%7D%5Cexp%5Cleft%5B-%5Cfrac%7B(y_%7Bi%7D-f%5Bx_%7Bi%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D%5Cright%5D%5Cright%5D%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A=%20%5Coperatorname*%7Bargmin%7D_%7B%5Cphi%7D%5Cleft%5B-%5Csum_%7Bi=1%7D%5E%7BI%7D%5Cleft(%5Clog%5Cleft%5B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E%7B2%7D%7D%7D%5Cright%5D-%5Cfrac%7B(y_%7Bi%7D-f%5Bx_%7Bi%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright)%5Cright%5D%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A=%20%5Coperatorname*%7Bargmin%7D_%7B%5Cphi%7D%5Cleft%5B-%5Csum_%7Bi=1%7D%5E%7BI%7D%5Cfrac%7B(y_%7Bi%7D-f%5Bx_%7Bi%7D,%5Cphi%5D)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%5Cright%5D%0A%5Cend%7Balign*%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A=%20%5Coperatorname*%7Bargmin%7D_%7B%5Cphi%7D%5Cleft%5B%5Csum_%7Bi=1%7D%5E%7BI%7D(y_%7Bi%7D-f%5Bx_%7Bi%7D,%5Cphi%5D)%5E%7B2%7D%5Cright%5D,%0A%5Cend%7Balign*%7D"></p>
<p>where we removed the first term between the second and third lines because it doesn’t depend on <img src="https://latex.codecogs.com/png.latex?%5Cphi">. We removed the denominator between the third and fourth lines, as this is just a constant positive scaling factor that does not affect the position of the minimum.</p>
<p>The result of these manipulations is the least squares loss function that we originally introduced when we discussed linear regression in chapter 2:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AL%5B%5Cphi%5D%20=%20%5Csum_%7Bi=1%7D%5E%7BI%7D(y_%7Bi%7D-f%5Bx_%7Bi%7D,%5Cphi%5D)%5E%7B2%7D.%0A%5Cend%7Balign*%7D"></p>
<p>We see that the least squares loss function follows naturally from the assumptions that the predictions are (i) independent and (ii) drawn from a normal distribution with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20f%5Bx_%7Bi%7D,%5Cphi%5D">.</p>
<p><img src="https://toniortiz18.github.io/portfolio/forblog/posts/images/LeastSquaresLoss.svg" class="img-fluid"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metacognitive Insight
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This example brilliantly connects probability theory with practical regression by: (1) showing how Gaussian assumptions naturally lead to least squares, (2) demonstrating careful mathematical reasoning through term elimination (constants don’t affect optimization), and (3) reinforcing the conceptual link between loss functions and probability distributions. The visual representation of the loss function grounds the abstract mathematics in concrete intuition.</p>
</div>
</div>
</div>


<!-- -->

</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <guid>https://toniortiz18.github.io/portfolio/forblog/posts/</guid>
  <pubDate>Mon, 31 Mar 2025 12:09:28 GMT</pubDate>
</item>
</channel>
</rss>
